{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Goals**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the project would be as follows:\n",
    "\n",
    "1: Write your own code (not an existing repo on the internet) that performs single shot ptychography using structured illumination as demonstrated in the papers by Levitan et al.\n",
    "\n",
    "2: answer the following question:\n",
    "\n",
    "a. What happens if the illumination (=probe) field has zero crossings? Is this a problem for the method? Do zero crossing result in non-measured points?\n",
    "\n",
    "b. suppose the method is simulated using 2x2 upsampling (that is, 2x2 probe pixels determine the average phase in a 1x1 super-pixel inside the object). Is it required that there is a relative phase shift between the 2x2 pixels inside the probe? Or can the modulation be amplitude-only (that is no phase shift, but an intensity variation)\n",
    "\n",
    "c. What is the minimum amount of upsampling needed?\n",
    "\n",
    "d. Is there a class of illumination functions for which the method fails?\n",
    "\n",
    "e. Try implementing regularization to the method. Please include L1, L2, and Total variation regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Ptychography**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illumination: A grid of partially overlapping beams, each beam approaches the sample from different angles. \n",
    "\n",
    "Reconstruction: Randomized probe imaging algorithm. \n",
    "\n",
    "General strategy: Divide the intensity pattern into a collection of individual, smaller diffraction patterns. The smaller patterns are centered on an individual beam and tagged with a corresponding translation at the same plane.\n",
    "\n",
    "X-ray problem: Difficult to generate a grid of identical beams with sufficient angular separation and uniform intensities. Grating equation: $d(\n",
    "\\sin\\theta_m) = m\\lambda$. Small wavelength leads to small grating period.\n",
    "\n",
    "Feature: It recovers both the probe and the object, so the probe is not needed to be known. But the division of the detector limits resolution by limiting the NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method used in the single-shot article:\n",
    "\n",
    "A purely iterative algorithm for single-shot ptychography which uses a pre-calibrated probe and operates on full diffraction patterns without partitioning them into a ptychography dataset.\n",
    "\n",
    "This method overcomes limitations of the general strategy that in cases where the object contains high frequency components, the scattering from neighboring beams does overlap and interfere, causing reconstructions to perform poorly.\n",
    "\n",
    "The forward model for this algorithm:\n",
    "\n",
    "$$I = \\sum_{n=1}^{N} \\left| \\mathcal{F} \\left\\{ P_n \\cdot \\mathcal{F}^{-1} \\left\\{ \\mathcal{U} \\cdot \\mathcal{F} \\left\\{\\exp(i T)\\right\\} \\right\\} \\right\\} \\right|^2$$\n",
    "\n",
    "$P_n$ is the discrete representation= of the nth mode of the pre-calibrated probe. $\\mathcal{U} is a zero padding operator, and $T$ is a low-resolution representation of the object's transmission function\n",
    "\n",
    "Zero-padding $\\mathcal{U}$ is a band-limiting constraint that stablizes the inverse problem.\n",
    "\n",
    "$T$ is constraint to be purely real to apply an additional phase-only constraint on the object. Allowing $T$ to be complex-valued can remove the phase-only constraint. The final object function is defined as:\n",
    "\n",
    "$$ O = \\exp(iT)$$\n",
    "\n",
    "The first step of the model is to upsample the low-resolution object $O$ by padding it with zeros in Fourier space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import fft\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_generation():\n",
    "\n",
    "    pass\n",
    "\n",
    "def probe_read(filepath=r'Papercode\\reconstructions\\e17965_1_00678_ptycho_reconstruction.h5'):\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        # print(\"Keys in the file:\", list(f.keys()))\n",
    "        dataset = f['probe']\n",
    "        probe = dataset[:]  # 读取整个数据集\n",
    "        # there are two probe modes\n",
    "        return probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sampling_fre(obj, f=2):\n",
    "    '''\n",
    "        down sampling the obj with factor f in Fourier space\n",
    "    '''\n",
    "    m, n = obj.shape\n",
    "    obj_fre = fft.fftshift(fft.fft2(obj))\n",
    "    m_d, n_d = m // f, n // f # downsampling size\n",
    "    \n",
    "    m_center ,n_center = m // 2, n // 2 # original center\n",
    "    m_d_half, n_d_half = n_d // 2, m_d // 2 \n",
    "    \n",
    "    #index of cropped area\n",
    "    m_start = m_center - m_d_half\n",
    "    m_end = m_start + m_d\n",
    "    n_start = n_center - n_d_half\n",
    "    n_end = n_start + n_d\n",
    "    \n",
    "    obj_fre_cropped = obj_fre[m_start:m_end, n_start:n_end]\n",
    "    \n",
    "    obj_downSampled = fft.ifft2(fft.ifftshift(obj_fre_cropped))\n",
    "    return obj_downSampled\n",
    "    \n",
    "    \n",
    "def down_sampling_spa(obj,f=2):\n",
    "    '''\n",
    "        down sampling the obj with factor f in spatial domain\n",
    "        block averaging is employed, average amplitude and phase\n",
    "    '''\n",
    "    m, n = obj.shape\n",
    "    \n",
    "    # crop obj to ensure integer times with the factor\n",
    "    m_crop = (m // f) * f\n",
    "    n_crop = (n // f) * f\n",
    "    obj_cropped = obj[:m_crop, :n_crop]\n",
    "\n",
    "    reshaped = obj_cropped.reshape(m_crop // f, f, n_crop // f, f)\n",
    "    obj_downSampled = reshaped.mean(axis=(1, 3))\n",
    "    \n",
    "    return obj_downSampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_array(array, pad_array):\n",
    "    pad_factortb = (np.shape(pad_array)[0] - np.shape(array)[0]) // 2\n",
    "    pad_factorrl = (np.shape(pad_array)[1] - np.shape(array)[1]) // 2\n",
    "    paded_array = np.pad(array, ((pad_factortb, pad_factortb),(pad_factorrl, pad_factorrl)))\n",
    "    return paded_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_model(obj, probe, f=2):\n",
    "    '''\n",
    "        the forward model generates the update diffraction field from the down sampled obj\n",
    "        in this simulation assumes only one probe \n",
    "    '''\n",
    "    obj_downSampled = down_sampling_fre(obj, f=2)\n",
    "    obj_fre = fft.fftshift(fft.fft2(obj_downSampled))\n",
    "    obj_frepad = pad_array(obj_fre, probe)\n",
    "    obj_pad = fft.ifft2(fft.ifftshift(obj_frepad))\n",
    "    update_diff_pattern = np.abs(fft.fftshift(fft.fft2((probe[0,:,:] * 0.756 + probe[1,:,:] * 0.244) * obj_pad))) ** 2\n",
    "\n",
    "    return update_diff_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reconstruction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a reconstruction, we start with an initial guess of the project function and use a forward model to simulate the corresponding diffraction pattern. \n",
    "\n",
    "Next, we calculate the normalized mean squared error between the measured diffraction amplitudes and a simulated diffraction pattern including a known detector background:\n",
    "\n",
    "$$ L = \\frac{1}{\\sum_{ij}I_{ij}} \\sum_{ij}\\left( \\sqrt{|\\tilde{E_{ij}}|^2 + B_{ij}} - \\sqrt{I_{ij}}\\right)^2$$\n",
    "\n",
    "From the equation above, we can write it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from chex import assert_equal_shape\n",
    "\n",
    "def loss_function(simulated: jnp.ndarray, background: jnp.ndarray, measured: jnp.ndarray) -> float:\n",
    "    '''\n",
    "    Calculates the normalized mean squared error (L) for the given simulated diffraction amplitudes E, detector background B, and the measured diffraction pattern I.\n",
    "\n",
    "    Args: \n",
    "        simulated (jnp.ndarray): The simulated diffraction pattern (Intensity) calculated from a forward model\n",
    "        backgroud (jnp.ndarray): The detector background (Intensity)\n",
    "        I (jnp.ndarray): The measured diffraction pattern (Intensity)\n",
    "    \n",
    "    Return:\n",
    "        float: The normalized mean squared error (L) \n",
    "    '''\n",
    "    # Assert that the three arrays have the same shape\n",
    "    assert_equal_shape([simulated, background, measured])\n",
    "\n",
    "    # Calculate the factor to normalize the error\n",
    "    factor = 1.0 / (jnp.sum(measured) + 1e-10)\n",
    "\n",
    "    # Calculate the simulated diffraction amplitudes\n",
    "    simulated_amp = jnp.sqrt(simulated + background)\n",
    "\n",
    "    # Calculate the measured diffraction amplitudes\n",
    "    measured_amp = jnp.sqrt(measured)\n",
    "\n",
    "    # Calculate the normalized mean squared error\n",
    "    loss = factor * jnp.sum((simulated_amp - measured_amp) ** 2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.19935277\n"
     ]
    }
   ],
   "source": [
    "simulated = jnp.array([[1.0, 2.0], [3.0, 4.0]])  # Simulated amplitudes (complex or real)\n",
    "background = jnp.array([[0.1, 0.1], [0.1, 0.1]])   # Background\n",
    "measured = jnp.array([[1.5, 4.5], [9.5, 16.5]])   # Measured intensities\n",
    "\n",
    "print(\"loss =\", loss_function(simulated, background, measured))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use automatic differentiation to calculate the Wirtinger derivative of the loss function with respect to the object guess $T$:\n",
    "\n",
    "$$ \\frac{\\delta L}{\\delta T} = \\frac{\\delta L}{\\delta |E_{ij}|^2} \\frac{\\delta |E_{ij}|^2}{\\delta T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "def derivative_loss_function_wrt_obj(obj: jnp.ndarray, probe: jnp.ndarray):\n",
    "    '''\n",
    "    Calculates the Wirtinger derivative of the loss function with respect to the object guess obj.\n",
    "\n",
    "    Args:\n",
    "        obj (jnp.ndarray): The object guess (complex or real)\n",
    "\n",
    "    Return:\n",
    "        jnp.ndarray: The derivative of the loss function with respect to the object guess obj\n",
    "    '''\n",
    "    # Calculate the derivative of the foward model wrt the object\n",
    "    d_forward_model = grad(forward_model(obj, probe, f=2))(obj)\n",
    "\n",
    "    # Calculate the derivative of the loss function wrt the simulated pattern\n",
    "    simulated = forward_model(obj, probe, f=2)\n",
    "    d_loss_function_wrt_simulated = grad(loss_function)(simulated)\n",
    "\n",
    "    # Calculate the derivative of the loss function wrt the object\n",
    "    d_loss_function_wrt_obj = d_loss_function_wrt_simulated * d_forward_model\n",
    "\n",
    "    return d_loss_function_wrt_obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we feed those derivatives into an update for the object guess $T_{guess}$ using the Adam algorithm. This process is repeated iteratively until the object converges.\n",
    "\n",
    "Adam is short for Adaptive Moment Estimation. It's a popular optimization algorithm used in machine learning to train models like neural networks. It's a type of stochastic gradient descent (SGD) that adapts the learning rate for each parameter.\n",
    "\n",
    "Adam maintains two moving averages for each parameter $\\theta$:\n",
    "\n",
    "1.First Moment (Mean): An estimate of the gradient, similar to momentum, denoted as:\n",
    "$$ m_{t} = \\beta_{1}m_{t-1} + (1 - \\beta_1)g_t $$\n",
    "where $g_t$ is the gradient at time step t, and $\\beta_1$ is typically set to 0.9.\n",
    "\n",
    "2.Second Moment (Variance): An estimate of the squared gradient, similar to RMSprop, denoted as:\n",
    "$$ v_t = \\beta_2v_{t-1} + (1-\\beta_2)g_t^2$$\n",
    "where $\\beta_2$ is typically set to 0.999.\n",
    "\n",
    "The bias-corrected first and second moments are: \n",
    "$$ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "$$ \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "\n",
    "The parameter update rule is then:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    " where $\\alpha$ is the learning rate, $\\epsilon$ is a small constant to prevent division by zero, and $t$ is the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "def adam_optimization(init_obj: jnp.ndarray, measured: jnp.ndarray, probe: jnp.ndarray, learning_rate: float, num_iterations: int) -> jnp.ndarray:\n",
    "    '''\n",
    "    Optimizes the object guess using the Adam optimizer.\n",
    "\n",
    "    Args:\n",
    "        init_obj (jnp.ndarray): The initial object guess (complex or real)\n",
    "        measured (jnp.ndarray): The measured diffraction pattern (Intensity)\n",
    "        learning_rate (float): The learning rate\n",
    "        num_iterations (int): The number of optimization iterations\n",
    "\n",
    "    Return:\n",
    "        jnp.ndarray: The optimized object guess\n",
    "    '''\n",
    "    # Initialize the Adam optimizer\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "\n",
    "    # Initialize the state of the optimizer\n",
    "    opt_state = optimizer.init(init_obj)\n",
    "\n",
    "    obj = init_obj                       \n",
    "    for _ in range(num_iterations):\n",
    "        # Calculate the gradient of the loss function wrt the object guess\n",
    "        grad_obj = derivative_loss_function_wrt_obj(obj, probe)\n",
    "\n",
    "        # Update the optimizer state\n",
    "        updates, opt_state = optimizer.update(grad_obj, opt_state, obj)\n",
    "\n",
    "        # Update the object guess\n",
    "        obj = optax.apply_updates(obj, updates)\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_read(filepath=r'Papercode\\cxi_files\\e17965_1_00677.cxi'):\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        data = f['entry_1/data_1/data'][:]\n",
    "        data = np.sum(data,axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_read()\n",
    "probe = probe_read()\n",
    "filename = r'Papercode\\reconstructions\\e17965_1_00677_ptycho_reconstruction.h5'\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    bcg = f['background'][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilize\n",
    "# initial_data = data * np.exp(1j * np.random.uniform(0, 2*np.pi, size=data.shape))\n",
    "initial_obj_guess = np.ones(np.shape(data))* np.exp(1j * np.random.uniform(0, 2*np.pi, size=data.shape))\n",
    "obj = np.copy(initial_obj_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward_model() missing 1 required positional argument: 'probe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m update_obj \u001b[38;5;241m=\u001b[39m \u001b[43madam_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mabs(update_obj))\n",
      "Cell \u001b[1;32mIn[42], line 25\u001b[0m, in \u001b[0;36madam_optimization\u001b[1;34m(init_obj, measured, learning_rate, num_iterations)\u001b[0m\n\u001b[0;32m     22\u001b[0m obj \u001b[38;5;241m=\u001b[39m init_obj                       \n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Calculate the gradient of the loss function wrt the object guess\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     grad_obj \u001b[38;5;241m=\u001b[39m \u001b[43mderivative_loss_function_wrt_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Update the optimizer state\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grad_obj, opt_state, obj)\n",
      "Cell \u001b[1;32mIn[41], line 14\u001b[0m, in \u001b[0;36mderivative_loss_function_wrt_obj\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mCalculates the Wirtinger derivative of the loss function with respect to the object guess obj.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    jnp.ndarray: The derivative of the loss function with respect to the object guess obj\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate the derivative of the foward model wrt the object\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m d_forward_model \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate the derivative of the loss function wrt the simulated pattern\u001b[39;00m\n\u001b[0;32m     17\u001b[0m simulated \u001b[38;5;241m=\u001b[39m forward_model(obj)\n",
      "    \u001b[1;31m[... skipping hidden 17 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\vspace\\Lib\\site-packages\\jax\\_src\\linear_util.py:370\u001b[0m, in \u001b[0;36m_get_result_paths_thunk\u001b[1;34m(_fun, _store, *args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;129m@transformation_with_aux2\u001b[39m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_result_paths_thunk\u001b[39m(_fun: Callable, _store: Store, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 370\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m   result_paths \u001b[38;5;241m=\u001b[39m [_clean_keystr_arg_names(path) \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m generate_key_paths(ans)]\n\u001b[0;32m    372\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _store:\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;66;03m# In some instances a lu.WrappedFun is called multiple times, e.g.,\u001b[39;00m\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;66;03m# the bwd function in a custom_vjp\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward_model() missing 1 required positional argument: 'probe'"
     ]
    }
   ],
   "source": [
    "update_obj = adam_optimization(jnp.array(obj), jnp.array(data), jnp.array(probe), 0.4, 1000)\n",
    "plt.imshow(np.abs(update_obj))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
